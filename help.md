#### Методы opencv

1. cv2.accumulateWeighted(src, dst, alpha) - функция находит среднее значение всех поданных ей
   кадров. ( https://cvexplained.wordpress.com/2020/04/17/running-average-model-background-subtraction/)
    * src: исходное изображение;
    * dst: накопитель или изображение назначения с теми же каналами, что и у исходного изображения. (объявляется
      первоначально);
    * alpha: вес входного изображения. Альфа определяет скорость обновления (как быстро накопитель "забывает" о более
      ранних изображениях). Если альфа имеет большее значение, то среднее изображение пытается уловить даже очень
      быстрые и короткие изменения в данных.
2. cv2.absdiff(src1, src2[, dst]) - вычисляет абсолютную разницу на элемент между двумя массивами или между массивом и
   скаляром.
   dst(I)=saturate(|src1(I)−src2(I)|)
   (https://docs.opencv.org/3.4/d2/de8/group__core__array.html#ga6fef31bc8c4071cbc114a758a2b79c14)
    * src1: первый входной массив или скаляр;
    * src2: второй входной массив или скаляр;
    * dst: выходной массив, имеющий тот же размер и тип, что и входные массивы.
3. cv2.threshold(src, thresh, maxval, type[, dst]) - применяет пороговое значение фиксированного уровня к каждому
   элементу массива. (https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html)
    * src: входной массив (многоканальный, 8-битный или 32-битный с плавающей точкой);
    * thresh: пороговое значение;
    * maxval: максимальное значение для использования с пороговыми типами THRESH_BINARY и THRESH_BINARY_INV;
    * type: тип порогового значения (см. ThresholdTypes).
    * dst: выходной массив того же размера и типа и с тем же количеством каналов, что и src.
4. cv2.findContours(image, mode, method[, contours[, hierarchy[, offset]]]) - находит контуры в бинарном
   изображении. (https://docs.opencv.org/4.x/d4/d73/tutorial_py_contours_begin.html)
    * image: 8-битное одноканальное изображение. Изображение рассматривается как двоичное. Если режим равен RETR_CCOMP
      или RETR_FLOODFILL, то на вход также может быть подано 32-битное целочисленное изображение меток (CV_32SC1);
    * mode: режим извлечения контура;
    * method: метод аппроксимации контура;
    * contours: обнаруженные контуры. Каждый контур хранится как вектор точек (например, std::vector[std::vector >);
    * hierarchy: необязательный выходной вектор (например, std::vector<cv::Vec4i>), содержащий информацию о топологии
      изображения. Он имеет столько элементов, сколько контуров;
    * offset: необязательное смещение, на которое сдвигается каждая точка контура.
5. Площадь контура задается функцией cv.contourArea() или по моментам, M['m00'].
6. cv2.VideoCapture(index, apiPreference = CAP_ANY) - открывает камеру для захвата
   видео. (https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html)
    * index: идентификатор устройства видеозахвата, которое необходимо открыть. Чтобы открыть камеру по умолчанию,
      используя бэкенд по умолчанию, просто передайте 0. (для обратной совместимости использование camera_id +
      domain_offset (CAP_*) верно, когда apiPreference равно CAP_ANY);
    * apiPreference: предпочитаемые бэкенды Capture API для использования. Может использоваться для принудительного
      использования конкретной реализации считывателя, если доступно несколько: например, cv::CAP_DSHOW или cv::CAP_MSMF
      или cv::CAP_V4L.
7. cv2.flip(src, flipCode[, dst]) - переворачивает двумерный массив вокруг вертикальной, горизонтальной или обеих
   осей. (https://docs.opencv.org/3.4/d2/de8/group__core__array.html#gaca7be533e3dac7feb70fc60635adf441)
    * src: входной массив;
    * flipCode: флаг для указания способа переворачивания массива; 0 означает переворачивание вокруг оси x, а
      положительное значение (например, 1) - вокруг оси y. Отрицательное значение (например, -1) означает
      переворачивание вокруг обеих осей;
    * dst: выходной массив того же размера и типа, что и src.
8. cv2.cvtColor(src, code[, dst[, dstCn]]) - преобразование изображения из одного цветового пространства в
   другое. (https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab)
    * src: входное изображение: 8-битное беззнаковое, 16-битное беззнаковое ( CV_16UC... ) или с плавающей точкой
      одинарной точности;
    * code: код преобразования цветового пространства (см. ColorConversionCodes);
    * dst: выходное изображение того же размера и глубины, что и src;
    * dstCn: количество каналов в конечном изображении; если параметр равен 0, количество каналов определяется
      автоматически из src и code.
9. cv2.GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]]) - размывает изображение с помощью фильтра
   Гаусса. (https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html)
    * src: входное изображение; изображение может иметь любое количество каналов, которые обрабатываются независимо, но
      глубина должна быть CV_8U, CV_16U, CV_16S, CV_32F или CV_64F;
    * ksize: размер гауссова ядра. ksize.width и ksize.height могут быть разными, но оба они должны быть положительными
      и нечетными. Или они могут быть нулевыми, и тогда они вычисляются из sigma;
    * sigmaX: гауссово стандартное отклонение ядра в направлении X;
    * sigmaY: стандартное отклонение ядра Гаусса в направлении Y; если sigmaY равна нулю, она устанавливается равной
      sigmaX, если обе сигмы нулевые, они вычисляются из ksize.width и ksize.height, соответственно (см.
      getGaussianKernel для деталей);
    * dst: выходное изображение того же размера и типа, что и src;
    * borderType: метод экстраполяции пикселей, см. BorderTypes. BORDER_WRAP не поддерживается.
10. cv2.putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]]) - отображает
    указанную текстовую строку на
    изображении. (https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga5126f47f883d730f633d74f07456c576)
    * img: изображение;
    * text: текстовая строка для рисования;
    * org: нижний левый угол текстовой строки на изображении;
    * fontFace: тип шрифта, см. HersheyFonts;
    * fontScale: коэффициент масштаба шрифта, который умножается на базовый размер шрифта;
    * color: цвет текста;
    * thickness: толщина линий, используемых для рисования текста;
    * lineType: тип линии. См. LineTypes;
    * bottomLeftOrigin: при значении true начало данных изображения находится в левом нижнем углу. В противном случае -
      в левом верхнем углу.
11. cv2.drawContours(image, contours, contourIdx, color[, thickness[, lineType[, hierarchy[, maxLevel[, offset]]]]]) -
    рисует очертания контуров или заполненные
    контуры. (https://docs.opencv.org/4.x/d4/d73/tutorial_py_contours_begin.html)
    * image: конечное изображение;
    * conturs: все входные контуры. Каждый контур хранится как вектор точек;
    * contourIdx: параметр, указывающий контур для отрисовки. Если он отрицательный, то рисуются все контуры;
    * color: цвет контуров;
    * thickness: толщина линий, которыми рисуются контуры. Если значение отрицательное (например, thickness=FILLED ), то
      рисуются внутренние контуры;
    * lineType: тип соединения линий. См. LineTypes;
    * hierarchy: необязательная информация об иерархии. Она нужна только в том случае, если вы хотите нарисовать только
      некоторые контуры (см. maxLevel );
    * maxLevel: максимальный уровень для отрисованных контуров. Если значение равно 0, рисуется только указанный контур.
      Если значение равно 1, функция отрисовывает контур(ы) и все вложенные контуры. Если 2, функция рисует контур, все
      вложенные контуры, все вложенные контуры и так далее. Этот параметр учитывается только при наличии иерархии.
    * offset: необязательный параметр сдвига контуров. Сдвигает все нарисованные контуры на указанное offset=(dx,dy).
12. cv2.imshow(winname, mat) - отображает изображение в указанном
    окне. (https://docs.opencv.org/4.x/d7/dfc/group__highgui.html#ga453d42fe4cb60e5723281a89973ee563)
    * winname: имя окна;
    * mat: изображение, которое будет показано.
13. cv2.imwrite(filename, img[, params]) - сохраняет изображение в указанный
    файл. (https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html#gabbc7ef1aa2edfaa87772f1202d67e0ce)
    * filename: имя файла;
    * img: (mat или вектор Mat) изображение или изображения для сохранения;
    * params: специфические параметры формата, закодированные в виде пар (paramId_1, paramValue_1, paramId_2,
      paramValue_2, ... .) см. cv::ImwriteFlags.
14. cv2.rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]]) - рисует простой, толстый или заполненный
    прямоугольник
    вверх-вправо. (https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga07d2f74cadcf8e305e810ce8eed13bc9)
    * img: изображение;
    * pt1: вершина прямоугольника;
    * pt2: вершина прямоугольника, противоположная pt1;
    * color: цвет прямоугольника или яркость (полутоновое изображение);
    * thickness: толщина линий, составляющих прямоугольник. Отрицательные значения, например FILLED, означают, что
      функция должна нарисовать заполненный прямоугольник;
    * lineType: тип линии. См. LineTypes;
    * shift: количество дробных битов в координатах точки.
15. cv2.waitKey([, delay]) - ожидает ключевого события бесконечно (когда delay≤0) или в течение миллисекунд, когда delay
    положителен. (https://docs.opencv.org/4.x/d7/dfc/group__highgui.html#ga5628525ad33f52eab17feebcfba38bd7)
    * delay; задержка в миллисекундах. 0 - это специальное значение, которое означает "навсегда".
16. cv2.destroyAllWindows() - уничтожает все окна
    HighGUI. (https://docs.opencv.org/4.x/d7/dfc/group__highgui.html#ga6b7fc1c1a8960438156912027b38f481)
17. cv2.VideoWriter(filename, fourcc, fps, frameSize, isColor = true) - записать
    видео.(https://docs.opencv.org/4.x/dd/d9e/classcv_1_1VideoWriter.html#ac3478f6257454209fa99249cc03a5c59)
    * filename: имя выходного видеофайла:
    * fourcc: 4-символьный код кодека, используемого для сжатия кадров. Например, VideoWriter::fourcc('P','I','M','1') -
      кодек MPEG-1, VideoWriter::fourcc('M','J','P','G') - кодек motion-jpeg и т.д. Список кодеков можно получить на
      странице MSDN. Бэкенд FFMPEG с контейнером MP4 нативно использует другие значения как код fourcc: см. ObjectType,
      поэтому вы можете получить предупреждение от OpenCV о преобразовании кода fourcc;
    * fps: фреймрейт создаваемого видеопотока;
    * frameSize: размер видеокадров;
    * isColor: если значение не равно нулю, кодер будет ожидать и кодировать цветные кадры, в противном случае он будет
      работать с полутоновыми кадрами.

---

#### Методы keras и tensorflow

1. tf.keras.preprocessing.image.ImageDataGenerator(
   featurewise_center=False,
   samplewise_center=False,
   featurewise_std_normalization=False,
   samplewise_std_normalization=False,
   zca_whitening=False,
   zca_epsilon=1e-06,
   rotation_range=0,
   width_shift_range=0.0,
   height_shift_range=0.0,
   brightness_range=None,
   shear_range=0.0,
   zoom_range=0.0,
   channel_shift_range=0.0,
   fill_mode='nearest',
   cval=0.0,
   horizontal_flip=False,
   vertical_flip=False,
   rescale=None,
   preprocessing_function=None,
   data_format=None,
   validation_split=0.0,
   interpolation_order=1,
   dtype=None
   ) - создание пакетов тензорных изображений с дополнением данных в реальном
   времени. (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)
    - featurewise_center: установить среднее значение входного сигнала равным 0 по всему набору данных в зависимости от
      особенностей;
    - samplewise_center: установить среднее значение каждой выборки в 0;
    - featurewise_std_normalization: разделить входные данные на среднее значение по набору данных, в зависимости от
      признака;
    - samplewise_std_normalization: разделить каждый входной сигнал на его среднеквадратичное значение;
    - zca_epsilon: эпсилон для отбеливания ZCA. По умолчанию 1e-6;
    - zca_whitening: применить ZCA-отбеливание;
    - rotation_range: диапазон степеней для случайных вращений;
    - width_shift_range:
        * float: доля от общей ширины;
        * 1-D массивоподобный: случайные элементы из массива;
        * int: целое число пикселей из интервала (-width_shift_range, +width_shift_range);
    - height_shift_range:
        * float: доля от общей высоты;
        * 1-D массивоподобный: случайные элементы из массива;
        * int: целое число пикселей из интервала (-height_shift_range, +height_shift_range);
    - brightness_range: кортеж или список из двух плавающих значений. Диапазон для выбора значения сдвига яркости;
    - shear_range: интенсивность сдвига (угол сдвига против часовой стрелки в градусах);
    - zoom_range: диапазон для произвольного масштабирования. Если плавающее значение,
      то [lower, upper] = [1-zoom_range, 1+zoom_range];
    - channel_shift_range: диапазон для случайного смещения каналов;
    - fill_mode: один из {"constant", "nearest", "reflect" или "wrap"}. По умолчанию - "nearest". Точки вне границ
      входных данных заполняются в соответствии с заданным режимом:
        * 'constant': kkkkkkkk|abcd|kkkkkkkkk (cval=k);
        * 'nearest': aaaaaaaa|abcd|dddddddd;
        * 'reflect': abcddcba|abcd|dcbaabcd;
        * 'wrap': abcdabcd|abcd|abcd|abcdabcd;
    - cval: значение, используемое для точек вне границ при fill_mode = "constant";
    - horizontal_flip: случайно переворачивает входные данные по горизонтали;
    - vertical_flip: случайное переворачивание входов по вертикали;
    - rescale: коэффициент масштабирования. По умолчанию равен None. Если None или 0, масштабирование не применяется,
      иначе мы умножаем данные на указанное значение (после применения всех других преобразований);
    - preprocessing_function: функция, которая будет применяться к каждому входному изображению. Функция будет запущена
      после изменения размера и увеличения изображения. Функция должна принимать один аргумент: одно изображение (тензор
      Numpy с рангом 3), и выдавать тензор Numpy той же формы;
    - data_format: формат данных изображения, либо "channels_first", либо "channels_last". Режим "channels_last"
      означает, что изображения должны иметь форму (образцы, высота, ширина, каналы), режим "channels_first" означает,
      что изображения должны иметь форму (образцы, каналы, высота, ширина);
    - validation_split: доля изображений, зарезервированных для валидации (строго между 0 и 1);
    - dtype: тип, который будет использоваться для сгенерированных массивов.
2. tf.keras.applications.vgg16.preprocess_input(x, data_format=None) - предварительно обрабатывает тензор или массив
   Numpy, кодирующий пакет
   изображений. (https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)
    * x: массив numpy.array с плавающей точкой или tf.Tensor, 3D или 4D с 3 цветовыми каналами, со значениями в
      диапазоне [0, 255]. Предварительно обработанные данные записываются поверх входных данных, если типы данных
      совместимы. Чтобы избежать такого поведения, можно использовать numpy.copy(x);
    * data_format: необязательный формат данных тензора/массива изображений. По умолчанию None, в этом случае
      используется глобальная настройка tf.keras.backend.image_data_format() (если вы ее не изменили, по умолчанию
      используется значение "channels_last").
3. flow_from_directory(
   directory,
   target_size=(256, 256),
   color_mode='rgb',
   classes=None,
   class_mode='categorical',
   batch_size=32,
   shuffle=True,
   seed=None,
   save_to_dir=None,
   save_prefix='',
   save_format='png',
   follow_links=False,
   subset=None,
   interpolation='nearest',
   keep_aspect_ratio=False
   ) - принимает путь к каталогу и генерирует партии дополненных
   данных. (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)
    - directory: путь к целевому каталогу. Она должна содержать по одному подкаталогу для каждого класса. Любые
      изображения PNG, JPG, BMP, PPM или TIF внутри дерева каталогов каждого из подкаталогов будут включены в генератор;
    - target_size: кортеж целых чисел (высота, ширина), по умолчанию (256,256). Размеры, до которых будут изменены все
      найденные изображения;
    - color_mode: одно из "grayscale", "rgb", "rgba". По умолчанию: "rgb". Будет ли изображение преобразовано в 1, 3 или
      4 канала;
    - classes: необязательный список подкаталогов классов (например, ['dogs','cats']). По умолчанию: None. Если список
      классов не указан, он будет автоматически выведен из имен подкаталогов/структуры каталога, где каждый подкаталог
      будет рассматриваться как отдельный класс (и порядок классов, который будет соответствовать индексам меток, будет
      буквенно-цифровым). Словарь, содержащий отображение имен классов на индексы классов, можно получить через атрибут
      class_indices;
    - class_mode: одно из "categorical", "binary", "sparse", "input" или None. По умолчанию: "categorical". Определяет
      тип возвращаемых массивов меток:
        * "categorical": будет представлять собой двумерные одноточечные кодированные метки;
        * "binary": одномерные двоичные метки, "sparse" - одномерные целочисленные метки;
        * "input": изображения, идентичные входным (в основном используется для работы с автоэнкодерами);
        * Если None, то метки не возвращаются (генератор будет выдавать только партии данных изображений, что удобно для
          использования с model.predict_generator()). В случае class_mode - None данные все равно должны находиться в
          подкаталоге каталога для корректной работы;
    - batch_size: размер партии данных (по умолчанию: 32);
    - shuffle: нужно ли перемешивать данные (по умолчанию: True) Если установлено значение False, данные сортируются в
      алфавитно-цифровом порядке;
    - seed: необязательное случайное зерно для перетасовки и преобразований;
    - save_to_dir: None или str (по умолчанию: None). Опционально позволяет указать каталог, в который будут сохраняться
      создаваемые дополненные изображения (полезно для визуализации того, что вы делаете);
    - save_prefix: префикс, который будет использоваться для имен файлов сохраняемых изображений (имеет значение, только
      если задано save_to_dir);
    - save_format: одно из "png", "jpeg", "bmp", "pdf", "ppm", "gif", "tif", "jpg" (актуально, только если задано
      save_to_dir). По умолчанию: "png";
    - follow_links: следует ли следовать "симлинкам" внутри подкаталогов класса (по умолчанию: False);
    - subset: подмножество данных ("обучение" или "проверка"), если в ImageDataGenerator задано validation_split;
    - interpolation: метод интерполяции, используемый для повторной выборки изображения, если целевой размер отличается
      от размера загруженного изображения. Поддерживаются следующие методы: "ближайшая" (nearest), "билинейная" (
      bilinear) и "бикубическая" (bicubic). По умолчанию используется "nearest";
    - keep_aspect_ratio: булево значение, нужно ли изменять размер изображений до заданного размера без искажения
      соотношения сторон. Перед изменением размера изображение обрезается по центру с заданным соотношением сторон.
4. tf.keras.Sequential(layers=None, name=None) - последовательная группировка линейного стека слоев в
   tf.keras.Model. (https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)
    * layers: необязательный список слоев для добавления в модель;
    * name: необязательное имя для модели.
5. add(layer) - добавляет экземпляр слоя на вершину стека
   слоев. (https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)
    * layer: экземпляр слоя.
6. tf.keras.layers.Conv2D(
   filters,
   kernel_size,
   strides=(1, 1),
   padding='valid',
   data_format=None,
   dilation_rate=(1, 1),
   groups=1,
   activation=None,
   use_bias=True,
   kernel_initializer='glorot_uniform',
   bias_initializer='zeros',
   kernel_regularizer=None,
   bias_regularizer=None,
   activity_regularizer=None,
   kernel_constraint=None,
   bias_constraint=None,
   **kwargs
   ) - слой двумерной свертки (например, пространственная свертка изображений). Этот слой создает ядро свертки, которое
   свертывается с входом слоя для получения тензора выходов. Если use_bias равно True, создается вектор смещения и
   добавляется к выходам. Наконец, если активация не None, она также применяется к выходам. При использовании этого слоя
   в качестве первого слоя в модели, укажите ключевой аргумент input_shape (кортеж целых чисел или None, не включает ось
   выборки), например input_shape=(128, 128, 3) для изображений 128x128 RGB в data_format="channels_last". Можно
   использовать None, если измерение имеет переменный
   размер. (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)
    * filters: размерность выходного пространства (т.е. количество выходных фильтров в свертке);
    * kernel_size: целое число или кортеж/список из 2 целых чисел, задающий высоту и ширину окна двумерной свертки.
      Может быть одним целым числом, чтобы указать одно и то же значение для всех пространственных измерений;
    * strides: целое число или кортеж/список из 2 целых чисел, задающий полосы свертки по высоте и ширине. Может быть
      одним целым числом, чтобы указать одно и то же значение для всех пространственных измерений. Указание любого
      значения stride != 1 несовместимо с указанием любого значения dilation_rate != 1;
    * padding: одно из "valid" или "same" (нечувствительно к регистру). "valid" означает отсутствие набивки. "same"
      означает равномерное заполнение нулями слева/справа или вверх/вниз от входных данных. Если padding="same" и
      strides=1, выход имеет тот же размер, что и вход;
    * data_format: одно из channels_last (по умолчанию) или channels_first. Упорядочивание размеров во входных данных.
      channels_last соответствует входам с формой (batch_size, height,width, channels), а channels_first соответствует
      входам с формой (batch_size, channels, height, width). По умолчанию используется значение image_data_format.
      Формат channels_first в настоящее время не поддерживается TensorFlow на CPU;
    * dilation_rate: целое число или кортеж/список из двух целых чисел, указывающий коэффициент расширения, используемый
      для расширенной свертки. Может быть одним целым числом, чтобы указать одно и то же значение для всех
      пространственных измерений. В настоящее время указание любого значения dilation_rate != 1 несовместимо с указанием
      любого значения stride != 1;
    * groups: положительное целое число, определяющее количество групп, на которые разбивается входной сигнал вдоль оси
      канала. Каждая группа свертывается отдельно с помощью фильтров / групп фильтров. На выходе получается конкатенация
      результатов всех групп вдоль оси канала. Входные каналы и фильтры должны быть кратны группам;
    * activation: функция активации для использования. Если ничего не указано, активация не применяется (см.
      keras.activations);
    * use_bias: использует ли слой вектор смещения;
    * kernel_initializer: инициализатор для матрицы весов ядра (см. keras.initializers). По умолчанию 'glorot_uniform';
    * bias_initializer: инициализатор для вектора смещения (см. keras.initializers). По умолчанию 'zeros';
    * kernel_regularizer: функция регуляризации, применяемая к матрице весов ядра (см. keras.regularizers);
    * bias_regularizer: функция регуляризации, применяемая к вектору смещения (см. keras.regularizers);
    * activity_regularizer: функция регуляризации, применяемая к выходу слоя (его "активации") (см. keras.regularizers);
    * kernel_constraint: функция ограничения, применяемая к матрице ядра (см. keras.constraints);
    * bias_constraint: функция ограничения, применяемая к вектору смещения (см. keras.constraints).
7. tf.keras.layers.MaxPool2D(
   pool_size=(2, 2),
   strides=None,
   padding='valid',
   data_format=None,
   **kwargs
   ) - операция максимального объединения для двумерных пространственных данных. Уменьшает выборку входных данных по
   пространственным измерениям (высоте и ширине), беря максимальное значение в окне ввода (размер которого определяется
   pool_size) для каждого канала входных данных. Окно сдвигается по страйдам вдоль каждого
   измерения. (https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)
    * pool_size: целое число или кортеж из 2 целых чисел, размер окна, по которому берется максимум. (2, 2) будет взято
      максимальное значение в окне объединения 2x2. Если указано только одно целое число, то для обоих измерений будет
      использоваться одинаковая длина окна;
    * strides: целое число, кортеж из 2 целых чисел или None. Указывает, на какое расстояние перемещается окно пулинга
      для каждого шага пулинга. Если None, то по умолчанию будет использоваться значение pool_size;
    * padding: одно из значений "valid" или "same" (нечувствительно к регистру). "valid" означает отсутствие отступов. "
      same" приводит к равномерному заполнению слева/справа или вверх/вниз от входных данных, так что выходные данные
      имеют ту же высоту/ширину, что и входные;
    * data_format: одно из channels_last (по умолчанию) или channels_first. Упорядочивание размеров во входных данных.
      channels_last соответствует входам с формой (партия, высота, ширина, каналы), а channels_first соответствует
      входам с формой (партия, каналы, высота, ширина). По умолчанию используется значение image_data_format.
8. tf.keras.layers.Flatten(data_format=None, **kwargs) - сглаживает входные данные. Не влияет на размер
   партии. (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)
    * data_format: одна из channels_last (по умолчанию) или channels_first. Упорядочивание размеров во входных данных.
      channels_last соответствует входам с формой (batch, ..., channels), а channels_first соответствует входам с
      формой (batch, channels, ...). По умолчанию используется значение image_data_format.
9. tf.keras.layers.Dense(
   units,
   activation=None,
   use_bias=True,
   kernel_initializer='glorot_uniform',
   bias_initializer='zeros',
   kernel_regularizer=None,
   bias_regularizer=None,
   activity_regularizer=None,
   kernel_constraint=None,
   bias_constraint=None,
   **kwargs
   ) - обычный слой NN с плотной связью. Реализует операцию: output = activation(dot(input, kernel) + bias), где
   activation - функция активации по элементам, переданная в качестве аргумента активации, kernel - матрица весов,
   созданная слоем, а bias - вектор смещения, созданный слоем (применимо, только если use_bias равен
   True). (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)
    * units: положительное целое число, размерность выходного пространства;
    * activation: функция активации для использования. Если ничего не указано, активация не применяется (т.е. "линейная"
      активация: a(x) = x);
    * use_bias: использует ли слой вектор смещения;
    * kernel_initializer: инициализатор для матрицы весов ядра;
    * bias_initializer: инициализатор для вектора смещения;
    * kernel_regularizer: функция регуляризации, применяемая к матрице весов ядра;
    * bias_regularizer: функция регуляризации, применяемая к вектору смещения;
    * activity_regularizer: функция регуляризации, применяемая к выходу слоя (его "активация");
    * kernel_constraint: функция ограничения, применяемая к матрице весов ядра;
    * bias_constraint: функция ограничения, применяемая к вектору смещения.
10. tf.keras.layers.Dropout(rate, noise_shape=None, seed=None, **kwargs) - применяет Dropout к входу. Слой Dropout
    случайным образом устанавливает входные единицы в 0 с частотой rate на каждом шаге во время обучения, что помогает
    предотвратить перестройку. Входы, не установленные в 0, масштабируются на 1/(1 - rate) так, что сумма по всем входам
    остается неизменной. Слой Dropout применяется только в том случае, если для обучения установлено значение True, т.е.
    значения не отбрасываются во время вывода. При использовании model.fit обучение будет соответствующим образом
    установлено в True автоматически, а в других контекстах можно явно установить kwarg в True при вызове
    слоя. (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)
    * rate: плавающее число от 0 до 1. Доля единиц входного сигнала для отбрасывания;
    * noise_shape: 1D целочисленный тензор, представляющий форму двоичной маски отсева, которая будет умножена на вход;
    * seed: целое число для использования в качестве случайной затравки.
11. Model.compile(
    optimizer="rmsprop",
    loss=None,
    metrics=None,
    loss_weights=None,
    weighted_metrics=None,
    run_eagerly=None,
    steps_per_execution=None,
    jit_compile=None,
    **kwargs
    ) - настраивает модель для обучения. (https://keras.io/api/models/model_training_apis/)
    * optimizer: имя оптимизатора или экземпляр оптимизатора. См tf.keras.optimizers;
    * loss: функция потерь. Может быть именем функции потерь или экземпляр. См. tf.keras.losses. Функция потерь - это
      любая вызываемая функция с сигнатурой loss = fn(y_true, y_pred), где y_true являются основными значениями
      истинности и y_pred являются предсказаниями модели. y_true должны иметь форму (batch_size, d0, .. dN) (за
      исключением случаев разреженных функций потерь, таких как разреженная категориальная кроссэнтропия, которая
      ожидает целочисленные массивы формы (batch_size, d0, .. dN-1)). y_pred должны иметь форму (batch_size, d0, .. dN).
      Функция потерь должна возвращать тензор с плавающей запятой.
    * metrics: список показателей, которые будут оцениваться моделью во время обучения и тестирования. Каждый из них
      может быть именем встроенной функции, функцией или tf.keras.metrics.Metric экземпляр. См. tf.keras.metrics;
    * loss_weights: необязательный список или словарь, определяющий скалярные коэффициенты для взвешивания вклада потерь
      в различные выходные данные модели. Значение потерь, которое будет минимизировано моделью, будет тогда взвешенной
      суммой всех отдельных потерь, взвешенных по loss_weights коэффициентам;
    * weighted_metrics: список показателей, подлежащих оценке и взвешиванию с помощью sample_weight или class_weight во
      время обучения и тестирования;
    * run_eagerly: по умолчанию имеет значение False. Если True, логика этой модели не будет заключена в tf.function.
      Рекомендуется оставить значение None, если ваша модель не может быть запущена внутри tf.function.run_eagerly =
      True не поддерживается при использовании tf.distribute.experimental.ParameterServerStrategy;
    * steps_per_execution: по умолчанию 1. Количество пакетов, запускаемых при каждом вызове tf.function;
    * jit_compile: Если True, скомпилируйте шаг обучения модели с помощью XLA. XLA — оптимизирующий компилятор для
      машинного обучения. jit_compile не включен по умолчанию. Эту опцию нельзя включить, если run_eagerly=True.
      Обратите внимание, что jit_compile=True необязательно работает для всех моделей. Дополнительные сведения о
      поддерживаемых операциях см. в документации XLA.
12. tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.1,
    patience=10,
    verbose=0,
    mode="auto",
    min_delta=0.0001,
    cooldown=0,
    min_lr=0,
    **kwargs
    ) - уменьшите скорость обучения, когда метрика перестала улучшаться. Модели часто выигрывают от снижения скорости
    обучения в 2-10 раз, когда обучение стагнирует. Этот обратный вызов отслеживает количество, и если не наблюдается
    улучшения для «терпения» количества эпох, скорость обучения
    снижается. (https://keras.io/api/callbacks/reduce_lr_on_plateau/)
    * monitor: количество, подлежащее контролю;
    * factor: фактор, на который будет снижаться скорость обучения new_lr = lr * factor;
    * patience: количество эпох без улучшения, после которых скорость обучения будет снижаться;
    * verbose: режим вербальности 0 или 1. Режим 0 является беззвучным, а режим 1 отображает сообщения, когда обратный
      вызов выполняет действие;
    * mode: одно из {'auto', 'min', 'max'}. В режиме 'min' скорость обучения будет уменьшена, когда контролируемое
      количество перестанет уменьшаться; в режиме 'max' она будет уменьшена, когда контролируемое количество перестанет
      увеличиваться; в режиме 'auto' направление автоматически определяется по названию контролируемого количества;
    * min_delta: порог для измерения нового оптимума, чтобы сосредоточиться только на значительных изменениях;
    * cooldown: количество эпох, которое необходимо выждать перед возобновлением нормальной работы после уменьшения lr;
    * min_lr: нижняя граница скорости обучения.
13. tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=0,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0,
    ) - остановите обучение, когда контролируемая метрика перестает улучшаться. Предположим, что целью тренировки
    является минимизация потерь. В этом случае отслеживаемая метрика будет 'loss', а режим - 'min'. Цикл обучения
    model.fit() в конце каждой эпохи будет проверять, не уменьшается ли больше потеря, учитывая min_delta и терпение,
    если применимо. Как только выясняется, что потери больше не уменьшаются, model.stop_training помечается True, и
    обучение завершается. Количество, которое необходимо отслеживать, должно быть доступно в logs dict. Чтобы сделать
    это, передайте потери или метрики в model.compile(). (https://keras.io/api/callbacks/early_stopping/)
    * monitor: количество, подлежащее мониторингу;
    * min_delta: минимальное изменение контролируемой величины для квалификации как улучшение, т.е. абсолютное изменение
      менее min_delta будет считаться отсутствием улучшения;
    * patience: количество эпох без улучшения, после которого обучение будет остановлено;
    * verbose: режим вербальности 0 или 1. Режим 0 является беззвучным, а режим 1 отображает сообщения, когда обратный
      вызов выполняет действие;
    * mode: одно из {"auto", "min", "max"}. В режиме "min" тренировка остановится, когда контролируемое количество
      перестанет уменьшаться; в режиме "max" - когда контролируемое количество перестанет увеличиваться; в режиме "auto"
      направление автоматически определяется по названию контролируемого количества.
    * baseline: базовое значение для контролируемой величины. Обучение прекратится, если модель не покажет улучшения по
      сравнению с базовым значением;
    * restore_best_weights: восстанавливать ли веса модели из эпохи с наилучшим значением контролируемой величины. Если
      False, то используются веса модели, полученные на последнем шаге обучения;
    * start_from_epoch: количество эпох, которое необходимо выждать перед началом наблюдения за улучшением. Это
      позволяет провести период разминки, в течение которого не ожидается улучшения и, следовательно, тренировка не
      будет остановлена.
14. Model.fit(
    x=None,
    y=None,
    batch_size=None,
    epochs=1,
    verbose="auto",
    callbacks=None,
    validation_split=0.0,
    validation_data=None,
    shuffle=True,
    class_weight=None,
    sample_weight=None,
    initial_epoch=0,
    steps_per_epoch=None,
    validation_steps=None,
    validation_batch_size=None,
    validation_freq=1,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    ) - обучает модель в течение фиксированного количества эпох (итераций на наборе
    данных). (https://keras.io/api/models/model_training_apis/)
    * x: входные данные. Это могут быть:
        - массив Numpy (или массив-подобный), или список массивов (в случае, если модель имеет несколько входов);
        - тензор TensorFlow или список тензоров (в случае, если модель имеет несколько входов);
        - словарь, отображающий имена входов на соответствующие массивы/тензоры, если модель имеет именованные входы;
        - набор данных tf.data. Должен возвращать кортеж из (входы, цели) или (входы, цели, веса_образцов);
        - генератор или keras.utils.Sequence, возвращающий (inputs, targets) или (inputs, targets, sample_weights);
        - tf.keras.utils.experimental.DatasetCreator, который оборачивает вызываемый объект, принимающий единственный
          аргумент типа tf.distribute.InputContext и возвращающий tf.data.Dataset;
    * y: целевые данные. Как и входные данные x, это может быть либо массив(ы) Numpy, либо тензор(ы) TensorFlow. Они
      должны соответствовать x. Если x - это набор данных, генератор или экземпляр keras.utils.Sequence, y не следует
      указывать (поскольку цели будут получены из x);
    * batch_size: количество выборок для обновления градиента. Если не указано, batch_size по умолчанию будет равен 32.
      Не указывайте batch_size, если ваши данные представлены в виде наборов данных, генераторов или экземпляров
      keras.utils.Sequence (поскольку они генерируют партии);
    * epochs: количество эпох для обучения модели. Эпоха - это итерация по всем предоставленным данным x и y;
    * verbose: 'auto', 0, 1 или 2. Режим вербальности. 0 = молчание, 1 = индикатор выполнения, 2 = одна строка за эпоху.
      Значение 'auto' по умолчанию равно 1 для большинства случаев, но 2 при использовании с ParameterServerStrategy;
    * callbacks: список экземпляров keras.callbacks.Callback. Список обратных вызовов для применения во время обучения.
      См. tf.keras.callbacks. Примечание tf.keras.callbacks.ProgbarLogger и tf.keras.callbacks.History обратные вызовы
      создаются автоматически и их не нужно передавать в model.fit;
    * validation_split: доля обучающих данных, которая будет использоваться в качестве проверочных данных. Модель
      выделяет эту часть обучающих данных, не обучается на них, а оценивает потери и любые метрики модели на этих данных
      в конце каждой эпохи. Данные для валидации выбираются из последних выборок в предоставленных данных x и y, перед
      перемешиванием;
    * validation_data: данные, на которых оцениваются потери и любые метрики модели в конце каждой эпохи. Модель не
      будет обучаться на этих данных;
    * shuffle: True или False, перемешивать ли обучающие данные перед каждой эпохой или строка (для 'batch'). Этот
      аргумент игнорируется, если x является генератором или объектом tf.data.Dataset. 'batch' - это специальная опция
      для работы с ограничениями данных HDF5; она перемешивает куски размером c партию;
    * class_weight: необязательный словарь, отображающий индексы классов (целые числа) на значение веса (float),
      используемое для взвешивания функции потерь (только во время обучения). Это может быть полезно для того, чтобы
      указать модели "уделять больше внимания" выборкам из недостаточно представленного класса;
    * sample_weight: необязательный массив Numpy с весами для обучающих выборок, используемый для взвешивания функции
      потерь (только во время обучения);
    * initial_epoch: эпоха, с которой следует начать тренировку (полезно для возобновления предыдущей тренировки);
    * steps_per_epoch: общее количество шагов (партий образцов) до объявления одной эпохи завершенной и начала следующей
      эпохи;
    * validation_steps: имеет значение, только если validation_data предоставлена и является набором данных tf.data.
      Общее количество шагов (партий образцов), которые нужно сделать до остановки при выполнении валидации в конце
      каждой эпохи;
    * validation_batch_size: количество образцов в партии валидации. Если не указано, по умолчанию будет использоваться
      значение batch_size;
    * validation_freq: имеет значение только в том случае, если предоставлены данные проверки. Integer или экземпляр
      collections.abc.Container (например, список, кортеж и т.д.);
    * max_queue_size: используется только для входа генератора или keras.utils.Sequence. Максимальный размер очереди
      генератора. Если не указано, max_queue_size по умолчанию будет равен 10;
    * workers: используется только для ввода генератора или keras.utils.Sequence. Максимальное количество процессов для
      запуска при использовании потоковой обработки на основе процессов. Если не указано, то по умолчанию для рабочих
      будет использоваться значение 1;
    * use_multiprocessing: используется только для ввода генератора или keras.utils.Sequence. Если True, используется
      потоковая обработка на основе процессов. Если не указано, use_multiprocessing по умолчанию будет иметь значение
      False.
15. Model.evaluate(
    x=None,
    y=None,
    batch_size=None,
    verbose="auto",
    sample_weight=None,
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False,
    return_dict=False,
    **kwargs
    ) - возвращает значение потерь и значения метрик для модели в тестовом режиме. Вычисления производятся партиями (см.
    аргумент batch_size). (https://keras.io/api/models/model_training_apis/)
    * x: входные данные. Это могут быть:
        - Массив Numpy (или массив-подобный), или список массивов (в случае, если модель имеет несколько входов);
        - Тензор TensorFlow, или список тензоров (в случае, если модель имеет несколько входов);
        - Словарь, отображающий имена входов на соответствующие массивы/тензоры, если модель имеет именованные входы;
        - Набор данных tf.data. Должен возвращать кортеж из (входы, цели) или (входы, цели, веса_образцов);
        - Генератор или keras.utils.Sequence, возвращающий (inputs, targets) или (inputs, targets, sample_weights);
    * y: целевые данные. Как и входные данные x, это может быть либо массив(ы) Numpy, либо тензор(ы) TensorFlow. Они
      должны соответствовать.;
      batch_size: количество образцов в одной партии вычислений. Если не указано, batch_size по умолчанию будет равен
      32. Не указывайте batch_size, если ваши данные имеют форму набора данных, генераторов или экземпляров
      keras.utils.Sequence (поскольку они генерируют партии);
    * verbose: "auto", 0, 1 или 2. Режим verbosity. 0 = без звука, 1 = индикатор выполнения, 2 = одна строка. Значение "
      auto" по умолчанию равно 1 для большинства случаев, и 2 при использовании с ParameterServerStrategy;
    * sample_weight: необязательный массив Numpy весов для тестовых выборок, используемых для взвешивания функции
      потерь. Вы можете передать либо плоский (одномерный) массив Numpy той же длины, что и входные выборки (отображение
      1:1 между весами и выборками), либо, в случае временных данных, вы можете передать двумерный массив с формой (
      samples, sequence_length), чтобы применить различные веса к каждому временному шагу каждой выборки;
    * steps: общее количество шагов (партий образцов) до объявления раунда оценки завершенным. Игнорируется при значении
      по умолчанию None. Если x является набором данных tf.data, а steps равно None, 'evaluate' будет выполняться до тех
      пор, пока набор данных не будет исчерпан. Этот аргумент не поддерживается с массивами входных данных:
    * callbacks: список экземпляров keras.callbacks.Callback. Список обратных вызовов для применения во время оценки.
      См. раздел "callbacks";
    * max_queue_size: используется только для генератора или входа keras.utils.Sequence. Максимальный размер очереди
      генератора. Если не указано, max_queue_size по умолчанию равен 10;
    * workers: используется только для генератора или входа keras.utils.Sequence. Максимальное количество процессов для
      запуска при использовании потоковой обработки на основе процессов. Если не указано, по умолчанию worker будет
      равен 1;
    * use_multiprocessing: используется только для генератора или входа keras.utils.Sequence. Если True, используется
      потоковая обработка на основе процессов. Если не указано, use_multiprocessing будет по умолчанию False;
    * return_dict: Если True, результаты потерь и метрики возвращаются в виде диктанта, где каждый ключ - это имя
      метрики. Если False, они возвращаются в виде списка.
16. Model.save(
    filepath,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=None,
    options=None,
    save_traces=True,
    ) - сохраняет модель в Tensorflow SavedModel или в отдельный файл
    HDF5. (https://keras.io/api/models/model_saving_apis/)
    * filepath: путь к SavedModel или H5 файлу для сохранения модели;
    * overwrite: перезаписывать ли все существующие файлы в целевом местоположении, или предоставить пользователю ручной
      запрос;
    * include_optimizer: если True, сохранять состояние оптимизатора вместе;
    * save_format: либо 'tf', либо 'h5', указывая, сохранять ли модель в Tensorflow SavedModel или HDF5. По умолчанию '
      tf' в TF 2.X и 'h5' в TF 1.X.:
    * signatures: подписи для сохранения вместе с SavedModel. Применимо только к формату 'tf'. Подробности см. в
      аргументе signatures в файле tf.saved_model.save;
    * options: (применяется только к формату SavedModel) tf.saved_model.SaveOptions объект, определяющий опции для
      сохранения в SavedModel;
    * save_traces: (применимо только к формату SavedModel) если включено, то SavedModel будет сохранять трассы функций
      для каждого слоя. Этот параметр можно отключить, чтобы сохранялись только конфигурации каждого слоя. По умолчанию
      установлено значение True. Отключение этого параметра уменьшит время сериализации и размер файла, но потребует,
      чтобы все пользовательские слои/модели реализовали метод get_config().

---

### Mediapipe

1. mp.solutions.holistic - конвейер MediaPipe Holistic объединяет отдельные модели для компонентов позы, лица и руки,
   каждый из которых оптимизирован для своей конкретной области. Многоступенчатый конвейер, который обрабатывает
   различные регионы, используя соответствующее региону разрешение изображения. Сначала оценивается поза человека с
   помощью детектора позы BlazePose и последующей модели ориентиров. Затем, используя найденные ориентиры позы, получаем
   три области интереса (ROI) для каждой руки и лица, а также используем модель повторного кадрирования для улучшения
   ROI. Затем обрезается входной кадр с полным разрешением до этих областей интереса и применяются модели лица и руки,
   специфичные для конкретной задачи, для оценки соответствующих ориентиров. Наконец, объединяем все ориентиры с
   ориентирами модели позы, чтобы получить 540+ ориентиров. (https://google.github.io/mediapipe/solutions/holistic.html)

##### Input

1. MIN_DETECTION_CONFIDENCE - минимальное значение достоверности ([0.0, 1.0]) из модели обнаружения человека, чтобы
   обнаружение считалось успешным. По умолчанию 0.5;
2. MIN_TRACKING_CONFIDENCE - минимальное значение достоверности ([0.0, 1.0]) из модели отслеживания ориентиров, чтобы
   ориентиры позы считались успешно отслеженными, иначе обнаружение человека будет вызвано автоматически на следующем
   входном изображении. Установка более высокого значения может повысить надежность решения за счет более высокой
   задержки. Игнорируется, если static_image_mode=true, где обнаружение человека просто выполняется для каждого
   изображения. По умолчанию 0.5;
3. STATIC_IMAGE_MODE - если установлено значение false, решение обрабатывает входные изображения как видеопоток. Он
   попытается обнаружить наиболее заметного человека на самых первых изображениях и после успешного обнаружения
   дополнительно локализует позу и другие ориентиры. Затем на последующих изображениях он просто отслеживает эти
   ориентиры, не вызывая другого обнаружения, пока не потеряет трек, уменьшая вычисления и задержку. Если задано
   значение true, функция обнаружения человека запускает каждое входное изображение, что идеально подходит для обработки
   пакета статических, возможно, не связанных изображений. По умолчанию false;
4. MODEL_COMPLEXITY - сложность модели ориентира для рук: 0или 1. Точность ориентиров, а также задержка вывода обычно
   повышаются со сложностью модели. По умолчанию - 1.
5. MAX_NUM_HANDS - максимальное количество рук для обнаружения. По умолчанию - 2.

##### Output

1. MULTI_HAND_LANDMARKS - коллекция обнаруженных рук, где каждая рука представлена в виде списка из 21 ориентира, и
   каждый ориентир состоит из x, y и z. x и y нормализованы до [0.0, 1.0] по ширине и высоте изображения соответственно.
   z представляет глубину ориентира, причем глубина на запястье является исходной точкой, и чем меньше значение, тем
   ближе ориентир к камере;
2. MULTI_HAND_WORLD_LANDMARKS - коллекция обнаруженных рук, где каждая рука представлена в виде списка из 21 ориентира в
   мировых координатах. Каждая достопримечательность состоит из xy, и z: реальные 3D координаты в метрах с началом
   координат в приблизительном геометрическом центре стрелки;
3. MULTI_HANDEDNESS - сбор данных о том, что руки были обнаружены (т. е. это левая или правая рука). Каждая раздача
   состоит из label и score. label является ли строка значением либо "Left" или "Right". score является расчетной
   вероятностью предсказанной раздачи и всегда больше или равна 0.5 (и противоположная рука имеет оценочную вероятность
   1 - score).